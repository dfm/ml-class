#!/usr/bin/env python
# encoding: utf-8
"""
Linear classifiers implemented for Homework 1 in Yann LeCun's ML class @ NYU

All original code by Daniel Foreman-Mackey

"""

from __future__ import division

__all__ = ['Perceptron', 'LinearRegression', 'LogisticRegression']

import numpy as np
import scipy as sp
import scipy.linalg

# Define a table layout
header = "Iter%11s%11s%11s%11s%11s%11s"%\
            ("tr-loss","tr-error","test-loss", "test-error", "full-loss",
                    "full-error")
layout  = "%(train-loss)10.4f %(train-error)10.4f "
layout += "%(test-loss)10.4f %(test-error)10.4f "
layout += "%(full-loss)10.4f %(full-error)10.4f"

class LinearClassifier(object):
    """
    The abstract base class for my Linear Classifier Machines

    Parameters
    ----------
    dataset : Dataset object
        A Dataset object generated by the dataset module

    """
    def __init__(self, dataset, eta=1.0, alpha=0.0, beta=0.0):
        self._dataset = dataset
        self._weights = np.ones(dataset.nvariables)
        self._bias = 0.0
        self._sum = 0.0

        self._eta, self._alpha, self._beta = eta, alpha, beta

    def run(self, sample):
        self._sum = self._bias + np.dot(self._weights, sample)
        loss = (self.loss(1), self.loss(-1))
        return 1 if loss[0] < loss[1] else -1

    def loss_function(self, label):
        raise NotImplementedError()

    def loss(self, label):
        w = np.linalg.norm(self._weights)
        return self.loss_function(label) \
                + 0.5 * self._alpha * (w**2 + self._bias**2) \
                + 0.5 * self._beta * np.abs(np.sum(self._weights) + self._bias)

    def delta(self, label):
        raise NotImplementedError()

    def train_sample(self, sample, label):
        a, b = self._alpha*self._eta, self._beta*self._eta
        delta = self._eta * self.delta(label)
        self._weights += delta * sample - a * self._weights - b
        self._bias    += delta - a - b

    def training_sweep(self):
        """
        Train the classifier on a dataset using stochastic gradient

        """
        train_in, train_out = self._dataset.training_set
        for i in range(self._dataset.size_train):
            self.run(train_in[i])
            self.train_sample(train_in[i], train_out[i])

    def train(self, maxiter=50, tol=5.25e-2, verbose=True):
        self.training_sweep()

        machine_name = type(self).__name__
        print "Optimizing %s" % machine_name
        print "========== "+"="*len(machine_name)
        print header

        if verbose:
            stats = self.stats()
            loss0 = stats['train-loss']
            print "   0",
            print layout%stats

        else:
            loss0, error0 = self.test(on_training_set=True)

        for i in xrange(maxiter):
            self.training_sweep()
            if verbose:
                stats = self.stats()
                loss = stats['train-loss']
                print "%4d"%(i+1),
                print layout%stats
                
            else:
                loss,error = self.test(on_training_set=True)

            # check for convergence
            if i > 1 and np.abs(loss-loss0) < tol:
                break
            loss0 = loss
        else:
            print "Warning: convergence criterion wasn't met after %d iterations"\
                    %maxiter

        if not verbose:
            stats = self.stats()
            print "%4d"%(i+1),
            print layout%stats

        return stats, i

    def test_sample(self, sample, label):
        """
        Test the prediction on a particular sample

        Parameters
        ----------
        sample : numpy.ndarray
            The test sample

        label : int
            The true label

        Returns
        -------
        loss : float
            The loss provided by this sample

        error : bool
            Is this prediction wrong?

        """
        r = self.run(sample)
        return (self.loss(label),
                ((r > 0 and label <= 0) or (r <= 0 and label > 0)))

    def test(self, on_training_set=False, on_full_set=False):
        """
        Test the prediction power of the optimized machine

        Parameters
        ----------
        on_training_set : bool, optional
            Return the test results calculated on the training set instead of
            the test set (default: False)

        Returns
        -------
        average_loss : float
            The loss function averaged over test samples

        fractional_error : float
            The fractional error on the test samples

        """
        errors, total_loss = 0, 0.0
        if on_full_set:
            test_in, test_out = self._dataset._inputs, self._dataset._outputs
        elif on_training_set:
            test_in, test_out = self._dataset.training_set
        else:
            test_in, test_out = self._dataset.test_set
        n = len(test_out)
        for i in range(n):
            loss, error = self.test_sample(test_in[i], test_out[i])
            errors += error
            total_loss += loss

        return total_loss/n, errors/n

    def stats(self):
        """
        Calculate the error statistics on the training, test and full datasets

        Returns
        -------
        stats : dict
            A dictionary with entries 'train-loss', 'train-error', 'test-loss',
            'test-error', 'full-loss' and 'full-error'. The '*-loss' entries
            give the mean of the loss function calculated on the particular
            dataset and '*-error' gives the fractional error on the same data.

        """
        stats = {}
        stats['train-loss'], stats['train-error'] = self.test(on_training_set=True)
        stats['test-loss'], stats['test-error'] = self.test()
        stats['full-loss'], stats['full-error'] = self.test(on_full_set=True)
        return stats

    def __str__(self):
        s =  "TRAIN: loss = %7.4f, error = %6.4f\n"%self.test(on_training_set=True)
        s += " TEST: loss = %7.4f, error = %6.4f"%self.test()
        return s

class Perceptron(LinearClassifier):
    """
    The simplest possible learning machine...

    """
    def loss_function(self, label):
        return (np.sign(self._sum) - label) * self._sum

    def delta(self, label):
        return label - np.sign(self._sum)

class LinearRegression(LinearClassifier):
    """
    Classification machine based on linear regression

    Includes both an online algorithm and a direct solution

    """
    def loss_function(self, label):
        return 0.5 * (self._sum - label)**2

    def delta(self, label):
        return (label - self._sum)

    def solve(self):
        """
        Direct solution of the linear system using LU decomposition

        Solution (\tilde{W}_{opt} = (w_0, \vec{w})^T) to

            (\tilde{X}^T \tilde{X}) \tilde{W} = \tilde{X}^T T

        Equation 4.16 from Bishop's "Pattern Recognition and Machine Learning"

        """
        x, t = self._dataset.training_set
        x = np.concatenate((np.atleast_2d(np.ones(x.shape[0])).T, x), axis=-1)
        A = np.dot(x.T, x) + np.diag(self._alpha*np.ones(x.shape[-1]))
        w = sp.linalg.lu_solve(sp.linalg.lu_factor(A), np.dot(x.T, t))
        self._weights = w[1:]
        self._bias = w[0]
        print "Direct solution of LinearRegression"
        print "====== ======== == ================"
        print header
        print "   0",
        print layout%(self.stats())

class LogisticRegression(LinearClassifier):
    """
    Classification machine based on logistic regression

    Includes both an online algorithm and a direct solution

    """
    def loss_function(self, label):
        return 2 * np.log(1+np.exp(-label*self._sum))

    def delta(self, label):
        return 2*label/(1+np.exp(label*self._sum))

